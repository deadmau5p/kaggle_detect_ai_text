{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import datasets\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizerFast, Trainer, TrainingArguments\nfrom datasets import load_dataset, DatasetDict\nimport torch\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-26T11:01:51.084154Z","iopub.execute_input":"2023-11-26T11:01:51.084397Z","iopub.status.idle":"2023-11-26T11:02:07.878135Z","shell.execute_reply.started":"2023-11-26T11:01:51.084373Z","shell.execute_reply":"2023-11-26T11:02:07.877063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_roberta():\n    roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\").to(\"cuda\")\n    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n\n    # training dataset\n\n    train_df = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\n    train_dataset = Dataset.from_pandas(train_df)\n    train_val_dataset = train_dataset.train_test_split(test_size=0.2)\n\n    tokenized_dataset = train_val_dataset.map(lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True), batched=True)\n\n    tokenized_dataset = tokenized_dataset.remove_columns([\"id\",\"prompt_id\",\"text\"])\n\n    tokenized_dataset = tokenized_dataset.rename_column(\"generated\", \"labels\")\n\n    training_args = TrainingArguments(\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        learning_rate=2e-5,\n        num_train_epochs=1,\n        weight_decay=0.01,\n        output_dir='./results',\n        logging_dir='./logs',\n        logging_steps=10,\n        report_to=\"none\"\n    )\n\n    trainer = Trainer(\n        model=roberta_model,                         # the instantiated ðŸ¤— Transformers model to be trained\n        tokenizer=tokenizer,                         # the instantiated ðŸ¤— Transformers tokenizer to be trained\n        args=training_args,                       # training arguments, defined above\n        train_dataset=tokenized_dataset[\"train\"],         # training dataset\n        eval_dataset=tokenized_dataset[\"test\"]             # evaluation dataset\n    )\n\n    trainer.train()\n    \n    trainer.evaluate()\n    \n    os.makedirs(\"/kaggle/working/finetuned_roberta/\", exist_ok=True)\n    \n    trainer.save_model(\"/kaggle/working/finetuned_roberta/\")\n    tokenizer.save_pretrained(\"/kaggle/working/finetuned_roberta/\")","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:02:07.879777Z","iopub.execute_input":"2023-11-26T11:02:07.881760Z","iopub.status.idle":"2023-11-26T11:02:07.891049Z","shell.execute_reply.started":"2023-11-26T11:02:07.881728Z","shell.execute_reply":"2023-11-26T11:02:07.890098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_roberta()","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:02:07.892533Z","iopub.execute_input":"2023-11-26T11:02:07.892871Z","iopub.status.idle":"2023-11-26T11:03:35.798275Z","shell.execute_reply.started":"2023-11-26T11:02:07.892846Z","shell.execute_reply":"2023-11-26T11:03:35.797468Z"},"trusted":true},"execution_count":null,"outputs":[]}]}