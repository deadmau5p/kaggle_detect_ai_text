{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-26T11:01:51.084397Z","iopub.status.busy":"2023-11-26T11:01:51.084154Z","iopub.status.idle":"2023-11-26T11:02:07.878135Z","shell.execute_reply":"2023-11-26T11:02:07.877063Z","shell.execute_reply.started":"2023-11-26T11:01:51.084373Z"},"trusted":true},"outputs":[],"source":["import datasets\n","import pandas as pd\n","from datasets import Dataset\n","from transformers import RobertaForSequenceClassification, RobertaTokenizerFast, Trainer, TrainingArguments\n","from datasets import load_dataset, DatasetDict, concatenate_datasets\n","import torch\n","import os\n","import wandb\n","wandb.init(project=\"kaggle-ai-detection\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T11:02:07.881760Z","iopub.status.busy":"2023-11-26T11:02:07.879777Z","iopub.status.idle":"2023-11-26T11:02:07.891049Z","shell.execute_reply":"2023-11-26T11:02:07.890098Z","shell.execute_reply.started":"2023-11-26T11:02:07.881728Z"},"trusted":true},"outputs":[],"source":["def train_roberta():\n","    roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\").to(device)\n","    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n","\n","    # training dataset\n","\n","    train_df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n","    dataset_list = []\n","        \n","    train_dataset = Dataset.from_pandas(train_df)\n","    train_val_dataset = train_dataset.train_test_split(test_size=0.2)\n","\n","    tokenized_dataset = train_val_dataset.map(lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True), batched=True)\n","\n","    tokenized_dataset = tokenized_dataset.remove_columns([\"prompt_name\",\"source\",\"text\", \"RDizzl3_seven\"])\n","\n","    #tokenized_dataset = tokenized_dataset.rename_column(\"generated\", \"labels\")\n","\n","    training_args = TrainingArguments(\n","        learning_rate=wandb.config.learning_rate,\n","        per_device_train_batch_size=wandb.config.batch_size,\n","        per_device_eval_batch_size=wandb.config.batch_size,\n","        num_train_epochs=wandb.config.epochs,\n","        weight_decay=wandb.config.weight_decay,\n","        load_best_model_at_end=True,\n","        output_dir='./results',\n","        logging_dir='./logs',\n","        logging_steps=100,\n","        report_to=\"wandb\"\n","    )\n","\n","    trainer = Trainer(\n","        model=roberta_model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","        tokenizer=tokenizer,                         # the instantiated ðŸ¤— Transformers tokenizer to be trained\n","        args=training_args,                       # training arguments, defined above\n","        train_dataset=tokenized_dataset[\"train\"],         # training dataset\n","        eval_dataset=tokenized_dataset[\"test\"]             # evaluation dataset\n","    )\n","\n","    trainer.train()\n","    \n","    metrics = trainer.evaluate()\n","    \n","    wandb.log(metrics)\n","\n","    # Close the wandb run\n","    wandb.finish()\n","    \n","    os.makedirs(\"/kaggle/working/finetuned_roberta_daigt/\", exist_ok=True)\n","\n","    trainer.save_model(\"/kaggle/working/finetuned_roberta_daigt/\")\n","    tokenizer.save_pretrained(\"/kaggle/working/finetuned_roberta_daigt/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T11:02:07.892871Z","iopub.status.busy":"2023-11-26T11:02:07.892533Z","iopub.status.idle":"2023-11-26T11:03:35.798275Z","shell.execute_reply":"2023-11-26T11:03:35.797468Z","shell.execute_reply.started":"2023-11-26T11:02:07.892846Z"},"trusted":true},"outputs":[],"source":["sweep_config = {\n","    'method': 'bayes',  # can be grid, random, bayes\n","    'metric': {\n","        'name': 'accuracy',\n","        'goal': 'maximize'   \n","    },\n","    'parameters': {\n","        'learning_rate': {\n","            'min': 1e-5,\n","            'max': 5e-4\n","        },\n","        'batch_size': {\n","            'values': [4, 8, 16]\n","        },\n","        \"epoch\":{\n","            \"values\": [1, 2, 3]\n","        }\n","    }\n","}\n","\n","sweep_id = wandb.sweep(sweep_config, project=\"kaggle-ai-detection\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wandb.agent(sweep_id, train_roberta)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":6888007,"sourceId":61542,"sourceType":"competition"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
